# ğŸš€ ç«ç‚¹æ£€æµ‹ç³»ç»Ÿéƒ¨ç½²æŒ‡å—

æœ¬æŒ‡å—å°†å¸®åŠ©æ‚¨åœ¨ä¸åŒç¯å¢ƒä¸­éƒ¨ç½²ç«ç‚¹æ£€æµ‹ç³»ç»Ÿã€‚

## ğŸ“‹ ç›®å½•

- [ç¯å¢ƒè¦æ±‚](#ç¯å¢ƒè¦æ±‚)
- [æ¨¡å‹è®­ç»ƒéƒ¨ç½²](#æ¨¡å‹è®­ç»ƒéƒ¨ç½²)
- [æ¨ç†æœåŠ¡éƒ¨ç½²](#æ¨ç†æœåŠ¡éƒ¨ç½²)
- [Docker éƒ¨ç½²](#dockeréƒ¨ç½²)
- [äº‘å¹³å°éƒ¨ç½²](#äº‘å¹³å°éƒ¨ç½²)
- [ç¡¬ä»¶ä¼˜åŒ–é…ç½®](#ç¡¬ä»¶ä¼˜åŒ–é…ç½®)
- [ç›‘æ§ä¸ç»´æŠ¤](#ç›‘æ§ä¸ç»´æŠ¤)

## ğŸ”§ ç¯å¢ƒè¦æ±‚

### åŸºç¡€ç¯å¢ƒ

- Python 3.8+
- CUDA 11.0+ (NVIDIA GPU)
- ROCm 5.0+ (AMD GPU)
- ç®—èƒ½ SDK (ç®—èƒ½ AI èŠ¯ç‰‡)

### æ¨èç¡¬ä»¶é…ç½®

#### è®­ç»ƒç¯å¢ƒ

- **GPU**: NVIDIA RTX 3080/4080 æˆ–æ›´é«˜
- **å†…å­˜**: 32GB+ RAM
- **å­˜å‚¨**: 500GB+ SSD
- **CPU**: Intel i7/AMD Ryzen 7 æˆ–æ›´é«˜

#### æ¨ç†ç¯å¢ƒ

- **GPU**: NVIDIA GTX 1660 æˆ–æ›´é«˜
- **å†…å­˜**: 16GB+ RAM
- **å­˜å‚¨**: 100GB+ SSD
- **CPU**: Intel i5/AMD Ryzen 5 æˆ–æ›´é«˜

## ğŸ‹ï¸ æ¨¡å‹è®­ç»ƒéƒ¨ç½²

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv fire_detection_env
source fire_detection_env/bin/activate  # Linux/Mac
# fire_detection_env\Scripts\activate   # Windows

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 2. æ•°æ®å‡†å¤‡

```bash
# åˆ›å»ºæ•°æ®ç›®å½•ç»“æ„
mkdir -p data/fire_dataset/{images,labels}

# æ•°æ®æ ¼å¼éªŒè¯
python -c "
from training.data_manager import DataManager
dm = DataManager('./data')
result = dm.validate_dataset('./data/fire_dataset/dataset.yaml')
print('æ•°æ®é›†éªŒè¯ç»“æœ:', result['valid'])
"
```

### 3. è®­ç»ƒé…ç½®

```yaml
# configs/training_config.yaml
model_name: yolov8n
input_size: 640
epochs: 100
batch_size: 16
learning_rate: 0.01
device: auto
save_dir: runs/train
workers: 8

# æ•°æ®å¢å¼º
augmentation: true
mixup: 0.0
copy_paste: 0.0

# ç¡¬ä»¶ä¼˜åŒ–
amp: true # æ··åˆç²¾åº¦è®­ç»ƒ
sync_bn: true # åŒæ­¥BatchNorm
```

### 4. å¯åŠ¨è®­ç»ƒ

```bash
# æ–¹å¼1: å‘½ä»¤è¡Œè®­ç»ƒ
python training/train.py \
    --images-dir ./data/fire_dataset/images \
    --labels-dir ./data/fire_dataset/labels \
    --epochs 100 \
    --batch-size 16 \
    --model yolov8n \
    --device auto

# æ–¹å¼2: é…ç½®æ–‡ä»¶è®­ç»ƒ
python training/train.py \
    --config configs/training_config.yaml \
    --data-config ./data/fire_dataset/dataset.yaml

# æ–¹å¼3: åˆ†å¸ƒå¼è®­ç»ƒ (å¤šGPU)
python -m torch.distributed.launch \
    --nproc_per_node=2 \
    training/train.py \
    --device 0,1 \
    --batch-size 32
```

### 5. æ¨¡å‹éªŒè¯ä¸å¯¼å‡º

```bash
# éªŒè¯æ¨¡å‹
python training/train.py \
    --validate \
    --model-path runs/train/weights/best.pt \
    --data-config ./data/fire_dataset/dataset.yaml

# å¯¼å‡ºONNXæ¨¡å‹
python -c "
from training.model_trainer import ModelTrainer
trainer = ModelTrainer()
trainer.export_model(
    model_path='runs/train/weights/best.pt',
    export_formats=['onnx', 'engine']
)
"
```

## ğŸŒ æ¨ç†æœåŠ¡éƒ¨ç½²

### 1. å¿«é€Ÿæœ¬åœ°éƒ¨ç½²

```bash
# å¯åŠ¨Web APIæœåŠ¡
python inference/deploy.py \
    --model-path ./models/fire_detection.onnx \
    --host 0.0.0.0 \
    --port 8000 \
    --workers 4

# æµ‹è¯•API
curl -X POST "http://localhost:8000/detect" \
     -H "Content-Type: multipart/form-data" \
     -F "file=@test_image.jpg"
```

### 2. é…ç½®æ–‡ä»¶éƒ¨ç½²

```yaml
# configs/deploy_config.yaml
host: 0.0.0.0
port: 8000
workers: 4
api_title: "ç«ç‚¹æ£€æµ‹API"
api_version: "1.0.0"

# æ¨¡å‹é…ç½®
model_path: ./models/fire_detection.onnx
confidence_threshold: 0.25
iou_threshold: 0.45
max_detections: 1000

# æ€§èƒ½é…ç½®
max_file_size: 10485760 # 10MB
batch_processing: false
cache_results: true

# å®‰å…¨é…ç½®
cors_origins: ["*"]
api_key_required: false
```

```bash
# ä½¿ç”¨é…ç½®æ–‡ä»¶å¯åŠ¨
python inference/deploy.py --config configs/deploy_config.yaml
```

### 3. è´Ÿè½½å‡è¡¡éƒ¨ç½²

```nginx
# nginxé…ç½®æ–‡ä»¶
upstream fire_detection_backend {
    server localhost:8001;
    server localhost:8002;
    server localhost:8003;
    server localhost:8004;
}

server {
    listen 80;
    server_name fire-detection.example.com;

    location / {
        proxy_pass http://fire_detection_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;

        # ä¸Šä¼ å¤§å°é™åˆ¶
        client_max_body_size 20M;

        # è¶…æ—¶è®¾ç½®
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
    }
}
```

```bash
# å¯åŠ¨å¤šä¸ªAPIå®ä¾‹
for port in 8001 8002 8003 8004; do
    python inference/deploy.py \
        --model-path ./models/fire_detection.onnx \
        --port $port &
done
```

## ğŸ³ Docker éƒ¨ç½²

### 1. æ„å»º Docker é•œåƒ

```dockerfile
# Dockerfile
FROM nvidia/cuda:11.8-runtime-ubuntu20.04

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    python3-dev \
    libglib2.0-0 \
    libsm6 \
    libxext6 \
    libxrender-dev \
    libgomp1 \
    && rm -rf /var/lib/apt/lists/*

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å¤åˆ¶é¡¹ç›®æ–‡ä»¶
COPY requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt

COPY . .

# åˆ›å»ºérootç”¨æˆ·
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["python3", "inference/deploy.py", "--host", "0.0.0.0", "--port", "8000"]
```

```bash
# æ„å»ºé•œåƒ
docker build -t fire-detection:latest .

# è¿è¡Œå®¹å™¨
docker run -d \
    --name fire-detection \
    --gpus all \
    -p 8000:8000 \
    -v $(pwd)/models:/app/models \
    -v $(pwd)/configs:/app/configs \
    fire-detection:latest
```

### 2. Docker Compose éƒ¨ç½²

```yaml
# docker-compose.yml
version: "3.8"

services:
  fire-detection-api:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models
      - ./configs:/app/configs
      - ./logs:/app/logs
    environment:
      - FIRE_DETECT_INFERENCE_DEVICE=cuda
      - FIRE_DETECT_INFERENCE_BACKEND=onnx_cuda
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - fire-detection-api
    restart: unless-stopped

volumes:
  models:
  logs:
```

```bash
# å¯åŠ¨æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f fire-detection-api

# åœæ­¢æœåŠ¡
docker-compose down
```

## â˜ï¸ äº‘å¹³å°éƒ¨ç½²

### 1. AWS éƒ¨ç½²

```yaml
# aws-deployment.yml (AWS ECS)
version: 1
applicationName: fire-detection-system
deploymentConfigName: CodeDeployDefault.ECSAllAtOnce

taskDefinition:
  family: fire-detection
  networkMode: awsvpc
  requiresCompatibilities:
    - EC2
    - FARGATE
  cpu: 2048
  memory: 4096

  containerDefinitions:
    - name: fire-detection-api
      image: your-account.dkr.ecr.region.amazonaws.com/fire-detection:latest
      portMappings:
        - containerPort: 8000
          protocol: tcp
      environment:
        - name: AWS_DEFAULT_REGION
          value: us-west-2
      logConfiguration:
        logDriver: awslogs
        options:
          awslogs-group: /ecs/fire-detection
          awslogs-region: us-west-2
          awslogs-stream-prefix: ecs
```

### 2. Azure éƒ¨ç½²

```yaml
# azure-container-instance.yml
apiVersion: 2019-12-01
location: eastus
name: fire-detection-group
properties:
  containers:
    - name: fire-detection-api
      properties:
        image: your-registry.azurecr.io/fire-detection:latest
        ports:
          - port: 8000
            protocol: TCP
        resources:
          requests:
            cpu: 2
            memoryInGB: 4
        environmentVariables:
          - name: FIRE_DETECT_INFERENCE_DEVICE
            value: cpu
  osType: Linux
  ipAddress:
    type: Public
    ports:
      - protocol: TCP
        port: 8000
  restartPolicy: Always
```

### 3. Google Cloud éƒ¨ç½²

```yaml
# gcp-cloud-run.yml
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: fire-detection-service
  namespace: default
spec:
  template:
    metadata:
      annotations:
        autoscaling.knative.dev/maxScale: "10"
        run.googleapis.com/cpu-throttling: "false"
    spec:
      containers:
        - image: gcr.io/your-project/fire-detection:latest
          ports:
            - containerPort: 8000
          resources:
            limits:
              cpu: 2000m
              memory: 4Gi
          env:
            - name: FIRE_DETECT_INFERENCE_DEVICE
              value: cpu
```

## âš¡ ç¡¬ä»¶ä¼˜åŒ–é…ç½®

### 1. NVIDIA GPU ä¼˜åŒ–

```bash
# å®‰è£…NVIDIA Container Toolkit
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update && sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker

# ç¯å¢ƒå˜é‡ä¼˜åŒ–
export CUDA_VISIBLE_DEVICES=0
export NVIDIA_TF32_OVERRIDE=1
export TORCH_CUDNN_V8_API_ENABLED=1

# TensorRTä¼˜åŒ–
python -c "
from training.model_trainer import ModelTrainer
trainer = ModelTrainer()
trainer.export_model(
    export_formats=['engine'],
    trt_fp16=True,
    trt_int8=False
)
"
```

### 2. AMD GPU ä¼˜åŒ–

```bash
# ROCmå®‰è£…
wget -qO - https://repo.radeon.com/rocm/rocm.gpg.key | sudo apt-key add -
echo 'deb [arch=amd64] https://repo.radeon.com/rocm/apt/5.4.3/ ubuntu main' | sudo tee /etc/apt/sources.list.d/rocm.list
sudo apt update && sudo apt install rocm-dev rocm-libs

# ç¯å¢ƒå˜é‡
export HIP_VISIBLE_DEVICES=0
export HSA_OVERRIDE_GFX_VERSION=10.3.0

# DirectMLä¼˜åŒ– (Windows)
pip install onnxruntime-directml
```

### 3. ç®—èƒ½ AI èŠ¯ç‰‡ä¼˜åŒ–

```bash
# SDKå®‰è£…
tar -xzf sophon-sdk-xxx.tar.gz
cd sophon-sdk
sudo ./install.sh

# ç¯å¢ƒé…ç½®
export SOPHON_SDK_PATH=/opt/sophon
export LD_LIBRARY_PATH=$SOPHON_SDK_PATH/lib:$LD_LIBRARY_PATH

# æ¨¡å‹è½¬æ¢
python -c "
from core.model_manager import ModelManager
mm = ModelManager()
mm.convert_to_bmodel(
    onnx_path='fire_detection.onnx',
    output_path='fire_detection_int8.bmodel',
    target='BM1684X',
    opt_level=2
)
"
```

## ğŸ“Š ç›‘æ§ä¸ç»´æŠ¤

### 1. æ€§èƒ½ç›‘æ§

```python
# monitoring/performance_monitor.py
import psutil
import GPUtil
from prometheus_client import start_http_server, Gauge

# å®šä¹‰æŒ‡æ ‡
cpu_usage = Gauge('cpu_usage_percent', 'CPUä½¿ç”¨ç‡')
memory_usage = Gauge('memory_usage_percent', 'å†…å­˜ä½¿ç”¨ç‡')
gpu_usage = Gauge('gpu_usage_percent', 'GPUä½¿ç”¨ç‡')
inference_time = Gauge('inference_time_seconds', 'æ¨ç†è€—æ—¶')
throughput = Gauge('throughput_fps', 'æ¨ç†ååé‡')

def collect_metrics():
    # CPUå’Œå†…å­˜
    cpu_usage.set(psutil.cpu_percent())
    memory_usage.set(psutil.virtual_memory().percent)

    # GPU
    gpus = GPUtil.getGPUs()
    if gpus:
        gpu_usage.set(gpus[0].load * 100)

if __name__ == "__main__":
    start_http_server(9090)
    while True:
        collect_metrics()
        time.sleep(10)
```

### 2. æ—¥å¿—é…ç½®

```yaml
# configs/logging.yaml
version: 1
formatters:
  default:
    format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
handlers:
  console:
    class: logging.StreamHandler
    level: INFO
    formatter: default
    stream: ext://sys.stdout
  file:
    class: logging.handlers.RotatingFileHandler
    level: DEBUG
    formatter: default
    filename: logs/fire_detection.log
    maxBytes: 10485760 # 10MB
    backupCount: 10
root:
  level: INFO
  handlers: [console, file]
loggers:
  inference:
    level: DEBUG
  training:
    level: INFO
```

### 3. å¥åº·æ£€æŸ¥

```python
# health_check.py
import requests
import time
import logging

def health_check(endpoint="http://localhost:8000/health"):
    try:
        response = requests.get(endpoint, timeout=5)
        if response.status_code == 200:
            return True
        else:
            logging.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {response.status_code}")
            return False
    except Exception as e:
        logging.error(f"å¥åº·æ£€æŸ¥å¼‚å¸¸: {e}")
        return False

def main():
    while True:
        if not health_check():
            # å‘é€å‘Šè­¦
            logging.critical("æœåŠ¡ä¸å¯ç”¨ï¼Œéœ€è¦é‡å¯!")
            # TODO: å®ç°è‡ªåŠ¨é‡å¯é€»è¾‘
        time.sleep(30)

if __name__ == "__main__":
    main()
```

### 4. è‡ªåŠ¨æ›´æ–°è„šæœ¬

```bash
#!/bin/bash
# update_deployment.sh

set -e

echo "å¼€å§‹æ›´æ–°éƒ¨ç½²..."

# å¤‡ä»½å½“å‰æ¨¡å‹
cp models/fire_detection.onnx models/fire_detection_backup_$(date +%Y%m%d_%H%M%S).onnx

# ä¸‹è½½æ–°æ¨¡å‹
wget -O models/fire_detection_new.onnx https://your-model-repo/latest/fire_detection.onnx

# éªŒè¯æ–°æ¨¡å‹
python -c "
from inference.fire_detector import FireDetector
detector = FireDetector('models/fire_detection_new.onnx')
print('æ¨¡å‹éªŒè¯é€šè¿‡')
"

# æ›¿æ¢æ¨¡å‹
mv models/fire_detection_new.onnx models/fire_detection.onnx

# é‡å¯æœåŠ¡
docker-compose restart fire-detection-api

echo "éƒ¨ç½²æ›´æ–°å®Œæˆ!"
```

## ğŸ”’ å®‰å…¨é…ç½®

### 1. API å®‰å…¨

```python
# æ·»åŠ APIå¯†é’¥è®¤è¯
from fastapi import HTTPException, Depends
from fastapi.security import HTTPBearer

security = HTTPBearer()

async def verify_token(token: str = Depends(security)):
    if token.credentials != "your-secret-api-key":
        raise HTTPException(status_code=401, detail="æ— æ•ˆçš„APIå¯†é’¥")
    return token
```

### 2. HTTPS é…ç½®

```bash
# ç”ŸæˆSSLè¯ä¹¦
openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -days 365 -nodes

# å¯åŠ¨HTTPSæœåŠ¡
python inference/deploy.py \
    --ssl-keyfile key.pem \
    --ssl-certfile cert.pem \
    --port 443
```

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚é‡åˆ°éƒ¨ç½²é—®é¢˜ï¼Œè¯·æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š

- ç³»ç»Ÿç¯å¢ƒ (OS, Python ç‰ˆæœ¬, GPU å‹å·)
- é”™è¯¯æ—¥å¿—
- é…ç½®æ–‡ä»¶å†…å®¹
- ç¡¬ä»¶èµ„æºä½¿ç”¨æƒ…å†µ

è”ç³»æ–¹å¼: ai@example.com
